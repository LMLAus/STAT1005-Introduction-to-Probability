---
title: "STAT1005 Data Analysis Condensed Cheat Sheet"
author: "Lisa Luff"
date: "5/28/2020"
output:
  pdf_document:
    includes:
      in_header: preamble.tex
  html_document:
    css: preamble.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Qualitative/Numerical Data

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Discrete Random Variables
### Sample Analysis
* Mean : ($\small \bar{x}$) = $\small \frac{1}{n} \sum_{i = 1}^{n}x_i$
* $\small E[X] = \mu_X = \sum_{x}x.p(x) = E[h(x)] = \sum h(x).p(x)$
  * Linearity of expectations
    * $\small E(aX + b) = aE(X) + b$
* In R:
  * Mean function  
$\small \to$ mean(x)
  * Equivalent function for $\small E[X]$  
$\small \to$ weighted.mean(y, p)
  
* Sample Standard Deviation: $\small s = \sqrt{\frac{1}{n - 1} \sum^{n}(x_i - \bar{x})^2}$ 
* Standard Deviation: $\small s = \sqrt{\frac{1}{n} \sum^{n}(x_i - \bar{x})^2}$
* $\small Var(X) = \sum[(x - \mu)^2 .p(x)] = E[(X - \mu)^2] = E[X^2] - E[X]^2$  
  *Linearity of expectations
    * $\small Var(aX + b) = |a|\sigma_x$
* In R:
  * Standard Deviation function  
$\small \to$ sd(x)
  * Variance function  
$\small \to$ var(x)

### Sample Probability Analysis
* Probability Mass Function (pmf): $\small p(x) = P(X = x)$  
  
* Cumulative Density Function (cdf): $\small F(x) = P(X \le x) = \sum p(y)$  
$\small \to P(a \le x \le b) = F(b) - F(a - 1)$  

* Confidence Interval: $\small \mu \pm Z*(\frac{s}{\sqrt{n}})$  
  * 90% CI - Z* = 1.645  
  * 95% CI - Z* = 1.96  
  * 99% CI - Z* = 2.576  

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Continuous Random Variables
### Sample Analysis
* Mean: $\small \bar{x} = E[X] = \int_{-\infty}^{\infty}x.f(x) dx$
* In R:
  * Mean function  
$\small \to$ mean(x)
  
* Standard Deviation: $\small \sigma = \sqrt{Var(X)}$
* $\small Var(X) = \int_{-\infty}^{\infty}(x - \mu)^2 .f(x) dx$
* In R:
  * Standard deviation function  
$\small \to$ sd(x)
  * Variance function  
$\small \to$ var(x)

### Sample Probability Analysis
* Probability Density Function (pdf): $\small f(x) = \int_{a}^{b}f(x) dx$
   
* Cumulative Density Function (cdf): $\small F(x) = P(X \le x) = \int_{-\infty}^{\infty}f(x) dx$  
$\small \to P(a \le X \le b) = F(a) - F(b)$
  
* Percentiles of a Continuous Distribution: $\small F[\eta(p)] = \int_{-\infty}^{\eta(p)}f(y) dy$
* In R:
  * Find the nth percentile  
$\small \to$ quantile(x, $\small n^{th}p$)  

* Confidence Interval: $\small \mu \pm Z*(\frac{\sigma}{\sqrt{n}})$  
  * 90% CI - Z* = 1.645  
  * 95% CI - Z* = 1.96  
  * 99% CI - Z* = 2.576  
  
#### Transforming Variables
* Transforming variables is when you rearrange the formula to find the value of a random variable X based on its probability.
  * 1. Need to use the cdf, so find the cdf if given a pdf
  * 2. Rearrange the cdf so that you have an equation that uses the value of F(X)/y to give X
  * 3. Enter the value into your new equation and now you know which random variable X will give you that value for y  

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Discrete Random Variables
### Probability Estimations Using Distributions
#### Central Limit Theorum
* As sample size increases (number of trials), the distribution of sample means will move towards a Normal shape, regardless of the distribution of the actual observations  

#### Normal Distribution
* Used when we know $\small \sigma$, otherwise use t distribution   
When $\small np$ and $\small n(1 - p) > 10$  
* $z = \frac{x - \bar{\mu}}{\sigma}$  
$\small \to$ or if you don't know $\small \mu$ and $\small \sigma$ use $\small \bar{x}$ and $\small s$
* The standard normal distribution has a mean of 0 and sd of 1
* In R:
  * Find p (probability)  
$\small \to$ pnorm(z)
Or $\small \to$ pnorm(x, mean = a, sd = b)
  * Find the z score  
$\small \to$ qnorm(p)

#### T Distribution
* Used when we don't know $\small \sigma$  
* Find t  statistic: $T_{n - 1} \sim t = \frac{\bar{x} - \mu_0}{s /\sqrt{n}}$  
  * With n - 1 degrees of freedom (df)
* In R:
  * Find the probability  
$\small \to$ pt(x, df)
  * Find t  
$\small \to$ qt(p, df)  
* In R:  
  * Find p (probability) 
$\small \to$ pt(x, df)
  * Find t  
$\small \to$ qt(p, df)  

#### Bernoulli Random Variable
* A random variable whose possible values are 0 and 1
* $\small P(x; a) = \Bigg \{ \begin{aligned} 1 & & & a \\ 0 & & & (1 - a) \end{aligned}$
* Basis of the other distributions                    

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Continuous Random Variables
### Probability Estimations Using Distributions
#### Standard Normal Distribution
* To calculate $\small P(a \le X \le b)$ when $\small X \sim N(\mu, \sigma^2)$  
$\to z = \int_{a}^{b}\frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-1}{2}(\frac{x - \mu}{\sigma}^2)} dx$
  * If $\small -\infty \le \mu \le \infty$, $\small \sigma > 0$
* Cdf denoted as: $\small \phi$
* In R:
  * Find p (probability)  
$\small \to$ pnorm(z)
OR $\small \to$ pnorm(x, mean = a, sd = b)
  * Find the z score  
$\small \to$ qnorm(p)  

#### Uniform Distribution
* When $\small f(x; A, B) = \Bigg \{ \begin{aligned} \frac{1}{A - B}, & & A \le x \le B \\ 0, & & otherwise \end{aligned}$
* $\small E[X] = \frac{A + B}{2}$
* $\small Var(X) = \frac{(B - A)^2}{12}$
* In R:
  * Find p (probability)  
$\small \to$ punif(x, min = a, max = b)
  * Find x for a given p  
$\small \to$ qunif(p, min = a, max = b)

#### Log Normal Distribution
* If $\small \ln(X) = N(\mu, \sigma)$
* In R:
  * Find p (probability)  
$\small \to$ plnorm(x, meanlog = a, sdlog = b)
  * Find x for a given p  
$\small \to$ qlnorm(p, meanlog = a, sdlog = b)

#### Gamma Distribution
* Generalised exponential function
* If $\small \alpha = 1$ and $\small \beta = \frac{1}{\lambda}$ see exponential distribution
  * Exponential function results from $\small \alpha = 1$ and $\small \beta = \frac{1}{\lambda}$
* pdf: $\small f(x; \alpha, \beta) = \frac{1}{\beta^{\alpha} \Gamma (\alpha)} x^{\alpha - 1} e^{\frac{-x}{\beta}}$, x > 0
  * $\small \Gamma (\alpha)$ is the Gamma function
  * $\small \alpha > 0$, and $\small \beta > 0$, if $\small \beta = 1$ it is a standard Gamma distribution
*Gamma function
  * $\small \Gamma (\alpha) = \int_{0}^{\infty} x^{\alpha - 1} e^{-x} dx$
  * $\small \Gamma(\frac{1}{2}) = \pi$
* In R:
  * Find p (probability)  
$\small \to$ pgamma(x, alpha, rate = beta, scale = 1/beta)
  * Find x for a given p  
$\small \to$ qgamma(p, alpha, rate = beta, scale = 1/beta)

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Discrete Random Variables
### Probability Estimations Using Distributions Continued
#### Binomial
* If you have $\small S$ - success or $\small F$ - failure
* Fixed $\small n$ - number of independent trials
* How many successes in $\small n$ number of trials?
* Exact for with replacement, approximate for without
* See hypergeometric if without replacement
* If $\small n > 50$ and $\small np > 5$ see Poisson
* pmf: $\small b(x; n, p) = \Bigg \{ \begin{aligned} {n \choose x} p^x (1 - p)^{n - x} & & x = 0, 1, .., n \\ 0 & & otherwise \end{aligned}$
* cdf: $\small B(x; n, p) = P(X \le x) = \sum_{y = 0}^x b(y; n, p)$, x = 1, 2, ..., n
* $\small E[X] = np$
* $\small Var(X) = np(1 - p) = npq$, where $\small q = 1 - p$
* In R:
  * Find p (probability)  
$\small \to$ pbinom(x, n, p)
  * Find x for a given p  
$\small \to$ qbinom(p, n, p)

#### Negative Binomial
* If you have $\small S$ - success or $\small F$ - failure
* Fixed $\small r$ - number of successes
* How many trials will it take to reach $\small r$ successes?
* If the $\small r^th$ success occurs on the $\small x^th$ trial, there must be (r - 1) successes in the first (x - 1) trials
* If $\small r = 1$, see geometric
* pmf: $\small nb(x; r, p) =$  
$\Bigg \{ \begin{aligned} {\binom{x - 1}{r - 1}} p^r (1 - p)^{x - r}, &  & x = r, r^{r + 1}, ...\\ 0 & & otherwise \end{aligned}$
* $\small E[X] = \frac{r}{p}$
* $\small Var(X) = \frac{r(1 - p)}{p^2}$
* In R:
  * Find p (probability)
$\small \to$ pnbinom(x, n, p, mu)
  * Find x for a given p  
$\small \to$ qbinom(p)  

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Continuous Random Variables
### Probability Estimations Using Distributions Continued
#### Exponential Disribution
* Inter-arrival times in a poisson process with a rate of $\small \lambda$ events per unit time
* pdf: $\small f(x; \lambda) = \lambda e^{-\lambda x}$, x > 0
* cdf: $\small 1 - e^{-\lambda x}$, x > 0
* $\small E[X] = \frac{1}{\lambda}$
* $\small Var(X) = \frac{1}{\lambda^2}$
* In R:
  * Find p (probability)  
$\small \to$ pexp(x, lambda)
  * Find x for a given p  
$\small \to$ qexp(p, lambda)

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}
## Discrete Random Variables
### Probability Estimations Using Distributions Continued
#### Geometric
* Negative binomial where $\small r = 1$
* pmf: $\small nb(x; 1, p) = (1 - p)^{x - 1} p, x = 1, 2..$
* If we redfine x as the number of failures:
  * $\small nb(x; 1, p) = (1 - p)^x p$, x = 0, 1, ...
* $\small E[X] = \frac{1}{p}$
* $\small Var(X) = \frac{1 - p}{p^2}$
* In R:
  * Find p (probability)  
$\small \to$ pgeom(x, p)
  * Find x for a given p  
$\small \to$ qgeom(p)

#### Hypergeometric
* Binomial without replacement
* N is finite population to be sampled
* M is number of successes in the population
* pmf: $P(X = x) = h(x; n, M, N) = \frac{{\binom{m}{x}}{\binom{N - M}{n - x}}}{\binom{N}{n}}$
* $\small E[X] = n\frac{M}{N}$
* $\small Var(X) = \frac{N - n}{N - 1} n \frac{M}{N} (1 - \frac{M}{N})$
* In R:
  * Find p (probability)   
$\small \to$ phyper(x, m, n, k)
  * Find x for a given p   
$\small \to$ qhyper(p)

#### Poisson
* Number of successes without a time period
* It is poisson if:
  * Events occur randomly in time
  * Independently and at a uniform rate
* Can be used as an approximation for a binomial if $\small n > 50$ and $\small np > 5$
  * The $\small E[X]$ of the binomial is used for the value of $\small lambda$ 
* pmf: $\small p(x; \lambda) = \frac{e^{-\lambda}\lambda^x}{x!}$, x = 0, 1, ...
* $\small E[X] = Var(X) = \lambda$
* In R:
  * Find p (probability)  
$\small \to$ ppois(x, lambda)
  * Find x for a given p  
$\small \to$ qpois(p)

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::::::

## Discrete and Continuous Random Variables

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

### Hypothesis Testing
* Set up hypotheses:  
$\small \to H_0 = \mu = \mu_0$  
$\small \to H_A = \mu \ne \mu_0$
  
* Find t statistic: $T_{n - 1} \sim t = \frac{\bar{x} - \mu_0}{s /\sqrt{n}}$
  
* Find the p-value:
  * $\small H_A: \mu \le \mu_0 = P(T \le t)$
  * $\small H_A: \mu \ge \mu_0 = P(T \ge t)$
  * $\small H_A: \mu \ne \mu_0 = 2P(T \ge |t|)$
* In R:
  * Find p-value  
$\small \to$ pt(t* , df = (n-1))

* Or you can find the t statistic, p-value and confidence interval in R
  * $\small \to$ t.test(Vector, $\small \mu_0$, alternative = "less""greater""two.sided", conf.level = X.XX)

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

### Comparing Population Means
* Sample populations must be independent
* They must be Normally distributed (or similar shape with no significant outliers)
  * Moderate skewness: $\small n_1 + n_2 \ge 15$
  * Strong skewness: $\small n_1 + n_2 \ge 40$
* Two-sample t statistic: $t = \frac{(\bar{x}_1 - \bar{x}_2)(\mu_1 - \mu_2)}{\sqrt{\frac{S_1^2}{N_1} + \frac{S_2^2}{N_2}}}$
* If you don't know $\small \mu$, just use $\small \bar{x}$
  
* Two-Sample Confidence Interval: $(\bar{x}_1 - \bar{x}_2) \pm t* \sqrt{\frac{S_1^2}{N_1} + \frac{S_2^2}{N_2}}$

* Think of it as a hypothesis test, where $\small H_0 = \bar{x}_1 - \bar{x}_2 = 0, \text{ and, } H_A = \bar{x}_1 - \bar{x}_2 \ne 0$  
  
* Both in R:
  * Find two sample t statistic and confidence interval  
$\small \to$ t.test(SampleVector, ComparisonVector, mu = 0, alternative = "less""greater""two.sided", conf.level = X.XX)

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Discrete Random Variables
### Joint Probability Distribution
#### Sample Probability Functions
* Joint Probability Mass Function (pmf): $\small p(x, y) = P(X = x, Y = y)$
  * $\small \to$ Created by $\small p(x)p(y)$ for each square
* In R:
  * Joint pmf  
$\small \to$ joint(X, Y)
  
* Marginal Probability Mass Function (marginal pmf):  
$\small \to$ Marginal pmf of x: $\small p_x (x) = \sum_{y} P(x, y)$  
$\small \to$ Marginal pmg of y: $\small P_y (y) = \sum_{x} P(x, y)$
  
* Joint Cumulative Density Function (cdf): $\small F(x, y) = P(x \le x, Y \le y) = \sum_{u \le x, v \le y} p(u, v)$ for all x, y

#### Independence of Random Variables
* X and Y are independent if:  
$\small \to P(x, y) = P_x (x).P_y (y)$, for all values of x and y

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Continuous Random Variables
### Joint Probability Distribution
#### Sample Probability Functions  
* Joint Probability Mass Function (pmf): $\small f(x, y) = P(a \le X \le b,c \le Y \le d) = \int_{a}^{b} \int_{c}^{d} f(x, y) dx dy$
* Calculate double integrals by:
  * 1. Inner integral: hold x constant, integrate over y
  * 2. Outer integral: Hold y constant, integrate over x
  * Doesn't matter which step is integrating for x or y
  
* Marginal Probability Mass Function (marginal pmf):  
$\small \to$ Marginal pmf of x: $\small p_x (x) = \int_{-\infty}^{\infty} f(x, y) dy$, for $\small -\infty \le X \le \infty$
$\small \to$ Marginal pmf of y: $\small P_y (y) = \int_{-\infty}^{\infty} f(x, y) dx$, for $\small -\infty \le Y \le \infty$
  
* Joint Cumulative Density Function (cdf): $\small F(x, y) = P(x \le x, Y \le y) = \sum_{u \le x, v \le y} p(u, v)$ for all x, y

#### Independence of Random Variables
* Independence of Random Variables
  * X and Y are independent if:  
$\small \to P(x, y) = P_x (x).P_y (y)$, for all values of x and y
  * AND the region of postive density has side parallel to the axis

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Discrete Random Variables
### Joint Probability Distribution Continued
#### Sample Probability Analysis
* $\small \mu_{h(x, y)} = E[h(x, y)] = \sum_{x} \sum_{y} h(x, y).p(x, y)$
* Covariance: $\small Cov(X, Y) = E[(X - \mu_x)(Y - \mu_y)]$
  * $\small \to = \sum_x \sum_y (X - \mu_x)(Y - \mu_y)p(x, y)$
  * $\small \to = E[X, Y] - E[X]E[Y]$
* If X and Y move together, covariance will be positive
* If x and Y move away, covariance will be negative
* If X and Y don't have a strong relation, covariance will near 0
* In R:
  * Covariance  
$\small \to$ cov(table or dataframe of X, Y)
  
* Correlation: $\small Corr(X, Y) = \frac{Cov(X, Y)}{\sigma_x \sigma_y}$
* Correlation is a linear measure of the strength of the relationship between -1 and 1
  * Not being correlated doesn't mean there isn't a relationship, it might be non-linear
* In R:
  * Correlation  
$\small \to$ cor(table or dataframe of X, Y)

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Continuous Random Variables
### Joint Probability Distribution Continued
#### Sample Probability Analysis
* $\small \mu_{h(x, y)} = E[h(x, y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} h(x, y).f(x, y) dx dy$
* Linearity of expectation:
  * $\small E[a_1 h_1 (X, Y) + a_2 h_2 (X, Y) + b] = a_1 E[h_1(X, Y)] + a_2 E[h_2(X.Y)] + b$
* If X and Y are independent:
  * $\small E[h(X, Y)] = E[f_x(X)f_y(Y)] = E[f_x(X)f_y(Y)]$
  
* Covariance: $\small Cov(X, Y) = E[(X - \mu_x)(Y - \mu_y)]$
  * $\small \to = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (X - \mu_x)(Y - \mu_y)f(X, Y) dx dy$
  * $\small \to = E[X, Y] - E[X]E[Y]$
* Properties of Covariance:
  * Cov(X, Y) = Cov(Y, X)
  * Cov(X, X) = Var(X)
  * For any random variable Z:
    * $\small \to Cov(aX + bY + c, Z) = aCov(X, Z) + bCov(Y, Z)$  
  
* Correlation: $\small Corr(X, Y) = \frac{Cov(X, Y)}{\sigma_x \sigma_y}$  

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Discrete Random Variables
### Joint Probability Distribution Continued  
#### Matched Pair Design
* Compare two populations in an experiment where they are paired
  * Case-control clinical trials
* Reduces two-sample t-test to a 1 sample
* More powerful (sensitive) test

* Pearson Correlation Coefficient
  * Covariance test for matched pair data
  * Strongly effected by outliers  
$\small \to r_{xy} = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$
  * Where:
    * $\small \to \bar{x} \sim \mu_x = n^{-1} \sum_{i = 1}^{i = n} x_i$
    * $\small \to \bar{y} \sim \mu_y = n^{-1} \sum_{i = 1}^{i = n} y_i$
    * $\small \to S_{X, Y} \sim Cov(X, Y)$   
    $\small = \sum_{i = 1}^n (X_i - \bar{x})(y_i - \bar{y})$
      * $\small \to = \sum_{i = 1}^n x_i y_i - n\bar{x}\bar{y}$
    * $\small \to S_{X, X} \sim \sigma_x^2 = \sum_{i = 1}^n (X_i - \bar{x})^2$
      * $\small \to = \sum_{i = 1}^n x_i - n\bar{x}^2$
    * $\small \to S_{Y, Y} \sim \sigma_y^2 = \sum_{i = 1}^n (y_i - \bar{y})^2$
      * $\small \to = \sum_{i = 1}^n y_i - n\bar{y}^2$  
* In R:  
$\small \to$ cov(table or dataframe of X, Y, paired = TRUE, method = 'pearson')
  
* Spearman's Correlation Coefficient
  * Based purely on the ranks of the data
    * From smallest to largest
  * Collected as numbers, then arranged in order
  * Not effected much by outliers  
$\small \to r_s = 1 - \frac{6 \sum_{i = 1}^n d_i^2}{n(n^2-1)}$
  * $\small \to d_i^2$ is the squared difference between the ranks of each category per variable
* In R:  
$\small \to$  cov(table or dataframe of X, Y, paired = TRUE, method = 'spearman')

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

\

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Discrete Random Variables
### Joint Probability Distribution Continued
#### Goodness-of-Fit Testing
* Tests is a probability model is an appropriate measure of the data collected, "close enough" to what we would expect to observe
* Chi-Squared Distribution: $\small \chi^2$  
  * Special case of Gamma distribution
    * When the degrees of freedom is $\small \nu$, then $\small \alpha = \frac{\nu}{2}$, and $\small \beta = 2$  
$\small \to \chi^2 = \sum_{i = 1}^k \frac{(o_i - e_i)^2}{e_i}$  = $\sum_{i = 1}^k \frac{(n_i - np_i)^2}{np_i}$
  * Where:
    * $\small o_i$ is the observed frequency
    * $\small e_i$ is the expected frequency
    * $\small n = \sum_{i = 1}^k n_i$
    * k - 1 degrees of freedom
* The size of the discrepency indicates how good a fit, to assess the size:
  * Find p-value
  * OR Compare to a critical value of $\small \alpha$ for $\small \chi_\alpha^2$
* In R:
  * Find p for $\small \chi^2$  
$\small \to$ pchisq($\chi_i^2$, k-1)
  * Find the critical value of $\small \alpha$  
$\small \to$ qchisq($\small \alpha$, k-1)  
OR chisq.test(Vector)

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}

## Continuous Random Variables
### Joint Probability Distribution Continued 
#### Goodness-of-Fit Testing
* See Goodness-of-Fit Discrete for more details
* Chi-Squared Distribution: $\small \chi^2$  
$\small \to \chi_{k - 1}^2 = \sum_{i = 1}^k \frac{(o_i - e_i)^2}{e_i}$  = $\sum_{i = 1}^k \frac{(n_i - np_i)^2}{np_i}$
  * Where cell properties are given by:
$\small \to p_i = P(a_{i - 1} \le X \le a_i) = \int_{a_{i - 1}}^{a_i} f(x) dx$
* Find p-value or use $\small \chi_\alpha^2$, where the critical variable is found using $\small 1 - \alpha$ (We want to find the top $\small \alpha$ percent)
* In R:
  * Find p for $\small \chi^2$
$\small \to$ pchisq($\chi_i^2$, k-1(df))
  * Find the critical value of $\small \alpha$ 
$\small \to$ pchisq($\small \alpha$, k-1(df))  
OR chisq.test(Vector)

:::

::::::

\newline

## Discrete and Continuous Random Variables  
### Two-Way Contingency Tables  
* For when there are multiple rows for multiple columns of information
* Multiple rows per column of information 
  * I rows (I $\small \le$ 2)
  * J columns

#### Testing for Independence  
* Use $\small \chi^2$ distribution
  * $\chi^2 = \sum \frac{(\text{observed count - expected count})^2}{\text{expected count}} \sim \chi_{(I - 1)(J - 1)}^2$
  * Where:
$\to \text{expected cell count} = \frac{\text{row total x column total}}{\text{expected count}}$
* In R:
  * Run the test
    * chisq.test(table or dataframe of X, Y)

\newpage 

# Categorical/Qualitative

## Nominal and Ordinal  

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

### Sample Analysis
* Sample Proportion: $\small \hat{p} \sim p$
  
* Standard Error: $\small \sqrt{p(1 - p)/n}$

### Calculating Sample Proportion
#### Defintions
* Ordered - Order of the outcomes matter. (DOG and GOD are different)
* Unordered - Order of the outcomes doesn't matter. (DOG and GOD are equivalent)
* With replacement - Even if chosen previously, equally likely to be chosen again
* Without replacement - Once chosen unable to be chosen again

#### Naive Definition of Probability
* All outcomes equally likely to occur
* $\small \hat{p} = P(A) = \frac{|A|}{|S|}$
* In R:
  * Find $\small P(A)$  
$\small \to$ Prob(A)  

#### General Product Rule
* Based on naive definition
* Ordered outcomes
* With replacement
* $\small n^k$

#### Permutation
* Based on naive definition
* Ordered outcomes
* Without replacement
* $\small \hat{p} = P_{k, n} = \frac{n!}{(n - k)!}$
* In R:  
$\small \to$ permutations(n, r)

#### Combination
* Based on naive definition
* Unordered outcomes
* Without replacement
* $\small \hat{p} = {n \choose k} = \frac{P_{k, n}}{n!} = \frac{n!}{(n - k)!k!}$
* In R:  
$\small \to$ choose(n, r)

#### Non-Naive Definition of Probability
* Probability of an event is $\small P(A)$
* $\small P(S) = \sum_{i = 1} P(a_i) = 1$

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}  

### Operations in Set Theory
* Based on Non-Naive Definition

* In R:
  * Union  
$\small \to$ union(a, b)
  * Intersection  
$\small \to$ intersect(a, b)

#### De Morgan's Laws
* $\small (A \cup B)^c = A^c \cap B^c$
* $\small (A \cap B)^c = A^c \cup B^c$

#### Additional Properties of Probability
* $\small P(A^c) = 1 - P(A)$
* If $\small A \subseteq B$, then $\small P(A) \le P(B)$
* $\small P(A \cup B) = P(A) + P(B) - P(A \cap B)$
* Property 1.
  * $\small P(S) = P(A \cup A^c) = P(A) + P(A^c) = 1$
* Property 2.
  * If $\small A \subseteq B$, then
  * $\small \to P(B) = P(A \cup(B \cap A^c)) = P(A) + P(B \cap A^c)$
  * $\small \to P(B) \ge P(A)$
* Property 3.
  * $\small P(B \cap A^c) = P(B) - P(A \cap B)$

#### Independence
* Two events are independent if
* $\small \to P(A \cap B) = P(A).P(B)$
* In R:
  * Test for independence  
$\small \to$ independent(X, Y)

:::

::::::

\newline

:::::: {.columns}

::: {.column width="48%" data-latex="{0.48\textwidth}"}

### Conditional Probability
* In R:
  * Find $\small P(A|B)$  
$\small \to$ prob(A, given = B)

#### Continuation of Set Theory
* $\small P(A|B) = \frac{P(A \cap B)}{P(B)}$

#### Independence
* If $\small P(B|A) = P(B)$, the two are independent.

#### Multiplication Rule
* $\small P( A \cap B) = P(A|B).P(B)$
* $\small P( A \cap B \cap C) = P(C|A \cap B).P(A \cap B)$
* OR $\small P( A \cap B \cap C) = P(C|A \cap B).P(B|A).P(A)$

#### The Law of Total Probability
* $\small P(B) = \sum_{i = 1}^k P(B|A_i).P(A_i)$

#### Bayes Theorum
* When $\small A_1, ..., A_k$ and mutually exclusive and exhaustive events
* $\small P(A_i|B) = \frac{P(B|A_i).(A_i)}{\sum_{i = 1}^ k P(B|A_i).P(A_i)}$

### Statistical Inference
#### Law of Large Numbers
* As sample size increases, the mean observed will get closer and closer to the true mean.

#### Normal Distribution
* Can be used as an estimate when $\small np$ and $\small n(1 - p) \ge 10$
* N = (p, $\small \frac{p(1 - p)}{n}$)
* In R:
  * Find p (proportion)  
$\small \to$ pnorm(z, $\small \hat{p}$, se)
  * Find the z score  
$\small \to$ qnorm(p)

#### Confidence Interval
* $\small \hat{p} = z*\sqrt{\hat{p}( 1 - \hat{p})/n}$
  * 90% CI - Z* = 1.645  
  * 95% CI - Z* = 1.96  
  * 99% CI - Z* = 2.576  

:::

::: {.column width="4%" data-latex="{0.04\textwidth}"}

\

:::

::: {.column width="48%" data-latex="{0.48\textwidth}"}    
#### Margin of Error
* m = $\small z*\sqrt{\hat{p}( 1 - \hat{p})/n}$
* Find n for a required margin of error
* $\small \to n = \frac{z*}{m}.p*(1 - p*)$
* Where p* is a guessed Value for the sample proportion
    * Margin of error will always $\small \le m$ if $\small p* = 0.5$ (round up)  
    
#### Hypothesis Testing
* Set up hypotheses:  
$\small \to H_0 = p = p_0$  
  * If $\small h_0$ is correct $\small \hat{p} \sim N(p, \frac{p(1 - p)}{n})$
$\small \to H_A = p \ne p_0$

* Find z statistic:
$\small z = \frac{(\hat{p} - p_0)}{\sqrt{p_0(1 - p_0)/n}}$

* Find p-value:
  * $\small H_A: p < p_0 = P(Z \le z)$
  * $\small H_A: p > p_0 = P(Z \ge z)$
  * $\small H_A: p \ne p_0 = 2P(Z \ge |z|)$

* Find p-value in R:
  * pnorm(Z*)

#### Further Analysis
* If you treat the data as discrete, or utilise the data as a table, you can use the same analysis as for discrete variables for two-way contingency tables

:::

::::::

\newpage  






  
  
  
